import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import EfficientNetB0
import os
import yaml, json
import shutil

# –î–ï–ë–ê–ì: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
print(f"–¢–µ–∫—É—â–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {os.getcwd()}")
print(f"–°—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ models: {os.path.exists('models')}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
with open('./params.yaml') as f:
    params = yaml.safe_load(f)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
EPOCHS = params['train']['epochs']
BATCH_SIZE = params['train']['batch_size']
DATA_DIR = "training_dataset"

# 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
datagen = ImageDataGenerator(
    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,
    validation_split=0.2
)

train_gen = datagen.flow_from_directory(
    DATA_DIR,
    target_size=(224, 224),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training'
)

val_gen = datagen.flow_from_directory(
    DATA_DIR,
    target_size=(224, 224),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation'
)

# 2. –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ EfficientNetB0
base_model = EfficientNetB0(
    weights='imagenet',
    include_top=False,
    input_shape=(224, 224, 3)
)

# –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º —Å–Ω–∞—á–∞–ª–∞
base_model.trainable = False

# –ü—Ä–æ—Å—Ç–∞—è –º–æ–¥–µ–ª—å –ø–æ–≤–µ—Ä—Ö EfficientNet
model = tf.keras.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(256, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(train_gen.num_classes, activation='softmax')
])

# 3. –ö–æ–º–ø–∏–ª—è—Ü–∏—è
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# 4. –û–±—É—á–µ–Ω–∏–µ
history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS,
    verbose=1
)

# 5. –°–û–ó–î–ê–ï–ú –ü–ê–ü–ö–£ –ü–ï–†–ï–î –°–û–•–†–ê–ù–ï–ù–ò–ï–ú
model_dir = "models"
if not os.path.exists(model_dir):
    print(f"–°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É {model_dir}")
    os.makedirs(model_dir)

# –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
model_path = os.path.join(model_dir, "monkey_classifier.h5")
print(f"–°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å –ø–æ –ø—É—Ç–∏: {model_path}")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
model.save(model_path)

# 6. –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω
if os.path.exists(model_path):
    print(f"‚úì –§–∞–π–ª —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ, —Ä–∞–∑–º–µ—Ä: {os.path.getsize(model_path)} –±–∞–π—Ç")
else:
    print(f"‚úó –§–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω!")

# 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
val_acc = history.history['val_accuracy'][-1]
val_loss = history.history['val_loss'][-1]

metrics = {
    "val_accuracy": float(val_acc),
    "val_loss": float(val_loss),
    "train_accuracy": float(history.history['accuracy'][-1]),
    "classes": train_gen.class_indices
}

metrics_path = "metrics.json"
with open(metrics_path, "w") as f:
    json.dump(metrics, f, indent=2)
    print(f"‚úì –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {metrics_path}")

print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
print(f"üìä –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {val_acc:.2%}")
print(f"üìÅ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")